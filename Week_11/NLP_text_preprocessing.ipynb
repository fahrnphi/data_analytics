{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sun dipped below the horizon, casting a warm glow across the tranquil lake. With a burst of laughter, friends gathered around the table, enjoying a delightful evening together. The old bookstore exuded a nostalgic charm, its shelves filled with tales of adventure and mystery.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'The sun dipped below the horizon, casting a warm glow across the tranquil lake.'\n",
    "d2 = 'With a burst of laughter, friends gathered around the table, enjoying a delightful evening together.'\n",
    "d3 = 'The old bookstore exuded a nostalgic charm, its shelves filled with tales of adventure and mystery.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the sun dipped below the horizon, casting a warm glow across the tranquil lake. with a burst of laughter, friends gathered around the table, enjoying a delightful evening together. the old bookstore exuded a nostalgic charm, its shelves filled with tales of adventure and mystery.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the sun dipped below the horizon casting a warm glow across the tranquil lake with a burst of laughter friends gathered around the table enjoying a delightful evening together the old bookstore exuded a nostalgic charm its shelves filled with tales of adventure and mystery'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'my', 'y', \"should've\", 'no', 'off', 'should', 'what', 'mustn', 'has', 'nor', 'than', 've', 'to', 'not', 'those', \"doesn't\", 'did', 'but', 'between', 'a', 'while', 'on', 'was', 'its', 'now', 'don', 'until', 'd', 'out', \"wasn't\", 'few', 't', \"don't\", \"didn't\", \"won't\", 'before', 'so', 'he', 'just', 'ma', 'that', 'again', 're', 'will', \"wouldn't\", 'hers', \"you'd\", 'can', 'the', 'through', \"weren't\", 'an', 'she', 'only', 'once', \"haven't\", 'when', 'any', 'yourselves', \"aren't\", 'shan', 'isn', 'be', 'had', 'doesn', 'you', 'her', 'themselves', 'with', 'haven', 'me', 'it', 'o', 'shouldn', 'have', 'more', 'do', 'll', \"hadn't\", 'above', 'wasn', 'itself', 'some', \"isn't\", 'i', \"shouldn't\", 'below', 'from', \"you're\", 'this', 'him', 'who', 'as', 'each', 'such', 'them', 'am', 'for', 'hadn', \"mustn't\", 'doing', 'over', 'during', 'why', 'how', 'their', 'because', \"you'll\", 'couldn', \"needn't\", 'won', 'your', 'they', 'in', 'if', 'up', 'needn', 'were', \"shan't\", 'further', 'aren', 'about', 'we', 'into', 'here', \"couldn't\", \"mightn't\", 'which', 'against', 'all', 'ourselves', 'myself', 's', 'weren', 'wouldn', 'at', 'are', 'most', \"it's\", 'where', 'ain', 'having', 'and', 'm', 'being', 'mightn', 'other', 'does', 'same', 'our', \"she's\", 'is', 'been', 'his', 'down', 'after', 'ours', 'yours', 'didn', 'hasn', 'by', 'very', \"that'll\", 'whom', 'both', 'of', \"you've\", 'theirs', 'then', 'herself', 'or', 'these', 'himself', 'there', 'under', 'too', 'yourself', \"hasn't\", 'own'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sun', 'dipped', 'horizon', 'casting', 'warm', 'glow', 'across', 'tranquil', 'lake', 'burst', 'laughter', 'friends', 'gathered', 'around', 'table', 'enjoying', 'delightful', 'evening', 'together', 'old', 'bookstore', 'exuded', 'nostalgic', 'charm', 'shelves', 'filled', 'tales', 'adventure', 'mystery']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['sun', 'dipped', 'horizon', 'casting', 'warm', 'glow', 'across', 'tranquil', 'lake', 'burst', 'laughter', 'friends', 'gathered', 'around', 'table', 'enjoying', 'delightful', 'evening', 'together', 'old', 'bookstore', 'exuded', 'nostalgic', 'charm', 'shelves', 'filled', 'tales', 'adventure', 'mystery'] \n",
      "\n",
      "After lemmatization:\n",
      "['sun', 'dip', 'horizon', 'cast', 'warm', 'glow', 'across', 'tranquil', 'lake', 'burst', 'laughter', 'friends', 'gather', 'around', 'table', 'enjoy', 'delightful', 'even', 'together', 'old', 'bookstore', 'exude', 'nostalgic', 'charm', 'shelve', 'fill', 'tales', 'adventure', 'mystery']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "corpus = ['sun dip horizon cast warm glow across tranquil lake burst laughter', \n",
    "          'friends gather around table enjoy delightful even together old bookstore', \n",
    "          'exude nostalgic charm shelve fill tales adventure mystery']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   across  adventure  around  bookstore  burst  cast  charm  delightful  dip  \\\n",
      "0       1          0       0          0      1     1      0           0    1   \n",
      "1       0          0       1          1      0     0      0           1    0   \n",
      "2       0          1       0          0      0     0      1           0    0   \n",
      "\n",
      "   enjoy  ...  mystery  nostalgic  old  shelve  sun  table  tales  together  \\\n",
      "0      0  ...        0          0    0       0    1      0      0         0   \n",
      "1      1  ...        0          0    1       0    0      1      0         1   \n",
      "2      0  ...        1          1    0       1    0      0      1         0   \n",
      "\n",
      "   tranquil  warm  \n",
      "0         1     1  \n",
      "1         0     0  \n",
      "2         0     0  \n",
      "\n",
      "[3 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   across tranquil  adventure mystery  around table  burst laughter  \\\n",
      "0                1                  0             0               1   \n",
      "1                0                  0             1               0   \n",
      "2                0                  1             0               0   \n",
      "\n",
      "   cast warm  charm shelve  delightful even  dip horizon  enjoy delightful  \\\n",
      "0          1             0                0            1                 0   \n",
      "1          0             0                1            0                 1   \n",
      "2          0             1                0            0                 0   \n",
      "\n",
      "   even together  ...  lake burst  nostalgic charm  old bookstore  \\\n",
      "0              0  ...           1                0              0   \n",
      "1              1  ...           0                0              1   \n",
      "2              0  ...           0                1              0   \n",
      "\n",
      "   shelve fill  sun dip  table enjoy  tales adventure  together old  \\\n",
      "0            0        1            0                0             0   \n",
      "1            0        0            1                0             1   \n",
      "2            1        0            0                1             0   \n",
      "\n",
      "   tranquil lake  warm glow  \n",
      "0              1          1  \n",
      "1              0          0  \n",
      "2              0          0  \n",
      "\n",
      "[3 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 29 \n",
      "\n",
      "The words in the corpus: \n",
      " {'tales', 'glow', 'fill', 'charm', 'adventure', 'dip', 'horizon', 'burst', 'delightful', 'old', 'shelve', 'mystery', 'nostalgic', 'lake', 'gather', 'cast', 'sun', 'around', 'laughter', 'even', 'exude', 'warm', 'friends', 'table', 'bookstore', 'together', 'tranquil', 'enjoy', 'across'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "   tales    glow   fill  charm  adventure     dip  horizon   burst  \\\n",
      "0  0.000  0.0909  0.000  0.000      0.000  0.0909   0.0909  0.0909   \n",
      "1  0.000  0.0000  0.000  0.000      0.000  0.0000   0.0000  0.0000   \n",
      "2  0.125  0.0000  0.125  0.125      0.125  0.0000   0.0000  0.0000   \n",
      "\n",
      "   delightful  old  ...  even  exude    warm  friends  table  bookstore  \\\n",
      "0         0.0  0.0  ...   0.0  0.000  0.0909      0.0    0.0        0.0   \n",
      "1         0.1  0.1  ...   0.1  0.000  0.0000      0.1    0.1        0.1   \n",
      "2         0.0  0.0  ...   0.0  0.125  0.0000      0.0    0.0        0.0   \n",
      "\n",
      "   together  tranquil  enjoy  across  \n",
      "0       0.0    0.0909    0.0  0.0909  \n",
      "1       0.1    0.0000    0.1  0.0000  \n",
      "2       0.0    0.0000    0.0  0.0000  \n",
      "\n",
      "[3 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "          tales:     0.4771\n",
      "           glow:     0.4771\n",
      "           fill:     0.4771\n",
      "          charm:     0.4771\n",
      "      adventure:     0.4771\n",
      "            dip:     0.4771\n",
      "        horizon:     0.4771\n",
      "          burst:     0.4771\n",
      "     delightful:     0.4771\n",
      "            old:     0.4771\n",
      "         shelve:     0.4771\n",
      "        mystery:     0.4771\n",
      "      nostalgic:     0.4771\n",
      "           lake:     0.4771\n",
      "         gather:     0.4771\n",
      "           cast:     0.4771\n",
      "            sun:     0.4771\n",
      "         around:     0.4771\n",
      "       laughter:     0.4771\n",
      "           even:     0.4771\n",
      "          exude:     0.4771\n",
      "           warm:     0.4771\n",
      "        friends:     0.4771\n",
      "          table:     0.4771\n",
      "      bookstore:     0.4771\n",
      "       together:     0.4771\n",
      "       tranquil:     0.4771\n",
      "          enjoy:     0.4771\n",
      "         across:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "    tales    glow    fill   charm  adventure     dip  horizon   burst  \\\n",
      "0  0.0000  0.0434  0.0000  0.0000     0.0000  0.0434   0.0434  0.0434   \n",
      "1  0.0000  0.0000  0.0000  0.0000     0.0000  0.0000   0.0000  0.0000   \n",
      "2  0.0596  0.0000  0.0596  0.0596     0.0596  0.0000   0.0000  0.0000   \n",
      "\n",
      "   delightful     old  ...    even   exude    warm  friends   table  \\\n",
      "0      0.0000  0.0000  ...  0.0000  0.0000  0.0434   0.0000  0.0000   \n",
      "1      0.0477  0.0477  ...  0.0477  0.0000  0.0000   0.0477  0.0477   \n",
      "2      0.0000  0.0000  ...  0.0000  0.0596  0.0000   0.0000  0.0000   \n",
      "\n",
      "   bookstore  together  tranquil   enjoy  across  \n",
      "0     0.0000    0.0000    0.0434  0.0000  0.0434  \n",
      "1     0.0477    0.0477    0.0000  0.0477  0.0000  \n",
      "2     0.0000    0.0000    0.0000  0.0000  0.0000  \n",
      "\n",
      "[3 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT', 'B-NP'),\n",
      " ('gentle', 'JJ', 'I-NP'),\n",
      " ('rustle', 'NN', 'I-NP'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('leaves', 'NNS', 'O'),\n",
      " ('overhead', 'VBP', 'O'),\n",
      " ('provided', 'VBD', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('soothing', 'JJ', 'I-NP'),\n",
      " ('backdrop', 'NN', 'I-NP'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('couple', 'NN', 'I-NP'),\n",
      " (\"'s\", 'POS', 'O'),\n",
      " ('quiet', 'JJ', 'B-NP'),\n",
      " ('conversation', 'NN', 'I-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('peaceful', 'JJ', 'I-NP'),\n",
      " ('park', 'NN', 'I-NP'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = '''The gentle rustle of leaves overhead provided a soothing backdrop to the couple's quiet conversation in the peaceful park.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### breafly explanation of 5 POS tags\n",
    "\n",
    "('The', 'DT', 'B-NP'): 'The' is a determiner (DT) and is the beginning (B) of a noun phrase (NP). <br>\n",
    "('gentle', 'JJ', 'I-NP'): 'gentle' is an adjective (JJ) and is inside (I) a noun phrase (NP). <br>\n",
    "('rustle', 'NN', 'I-NP'): 'rustle' is a noun (NN) and is inside (I) a noun phrase (NP). <br>\n",
    "('of', 'IN', 'O'): 'of' is a preposition (IN) and is outside (O) any named entity. <br>\n",
    "('leaves', 'NNS', 'O'): 'leaves' is a plural noun (NNS) and is outside (O) any named entity. <br>\n",
    "('overhead', 'VBP', 'O'): 'overhead' is a verb (VBP) and is outside (O) any named entity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.2.0-1016-azure\n",
      "Datetime: 2023-11-28 07:34:31\n",
      "Python Version: 3.10.13\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
